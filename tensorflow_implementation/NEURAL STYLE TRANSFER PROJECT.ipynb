{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project is illustrating how to use neural style transfer for both images and videos:\n",
    "> The necessary libaries and framework would be put in the requirement.txt but i'll still indicate them below:\n",
    "* matplotlib.pyplot\n",
    "* tensorflow\n",
    "* tensorflow_hub\n",
    "* numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\anaconda3\\envs\\newenv001\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\hp\\anaconda3\\envs\\newenv001\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "c:\\Users\\hp\\anaconda3\\envs\\newenv001\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libaries\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Style Transfer for images:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_view_image(content_image_path, style_image_path, save_path=None):\n",
    "    content_image = plt.imread(content_image_path)\n",
    "    style_image = plt.imread(style_image_path)\n",
    "    # Scale the images to ensure they are within the range of [0, 1]\n",
    "    content_image = content_image.astype(np.float32)[np.newaxis, ...] / 255.\n",
    "    style_image = style_image.astype(np.float32)[np.newaxis, ...] / 255.\n",
    "\n",
    "    # Resize the style image\n",
    "    style_image = tf.image.resize(style_image, [256, 256])\n",
    "    # Load the module\n",
    "    hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "    outputs = hub_module(tf.constant(content_image), tf.constant(style_image))\n",
    "    stylized_image = outputs[0]\n",
    "    # Convert the tensor to a NumPy array\n",
    "    stylized_image_array = stylized_image.numpy()\n",
    "\n",
    "    # Ensure values are within the valid range [0, 1]\n",
    "    stylized_image_array = np.clip(stylized_image_array, 0, 1)\n",
    "    # Visualize the outputs\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(content_image[0])\n",
    "    plt.title('Content Image')\n",
    "    plt.axis(False)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(style_image[0])\n",
    "    plt.title('Style Image')\n",
    "    plt.axis(False)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(stylized_image_array[0])\n",
    "    plt.title('Stylized Image')\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "\n",
    "    # Save the stylized image if a save path is provided\n",
    "    if save_path is not None:\n",
    "        plt.imsave(save_path, stylized_image_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_view_image('/content/drive/MyDrive/Neural Transfer Project/Content/spider-man-across-5120x2880-10140.jpg',\n",
    "                        '/content/drive/MyDrive/Neural Transfer Project/Style/frida.jpg',\n",
    "                        '/content/drive/MyDrive/Neural Transfer Project/Generated Output/stylized_image.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The purpose of adding the `np.new_axis` is to enable the module, which has been trained to process data in batches, to effectively handle individual data samples as if they were part of a batch. This adjustment ensures compatibility between the module's expectations and the input data, allowing seamless processing and accurate results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adjusting the size of the style image is necessary because the module was originally trained on style images with dimensions of (256,256) pixels.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I adjusted the dimensions of the `style` image to 256 by 256 pixels, aligning with the size used during training of the style transfer network. Meanwhile, the `content` image size can be customized according to your preferences.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Style Transfer for imported Videos from user device:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_display_video(video_path, style_image_path):\n",
    "    '''This function preprocesses the video and applies style transfer'''\n",
    "\n",
    "    # Load our style transfer module\n",
    "    hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "\n",
    "    # Capture the video file and transform it into a VideoCapture object\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Preprocess the style image\n",
    "    style_image = plt.imread(style_image_path)\n",
    "    style_image = style_image.astype(np.float32)[np.newaxis, ...] / 255.\n",
    "    style_image = tf.image.resize(style_image, [256, 256])\n",
    "\n",
    "    stylized_frames = []  # List to store stylized frames\n",
    "\n",
    "    # Create a while loop to read and stylize each frame of the video\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # preprocess the video\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = frame.astype(np.float32)[np.newaxis, ...] / 255\n",
    "        frame = tf.image.resize(frame, [256, 256])\n",
    "\n",
    "        # Apply stylization\n",
    "        stylized_output = hub_module(tf.constant(frame), tf.constant(style_image, dtype=tf.float32))\n",
    "        stylized_frame = (stylized_output[0].numpy()[0] * 255).astype(np.uint8)\n",
    "        stylized_frame = cv2.cvtColor(stylized_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        stylized_frames.append(stylized_frame)\n",
    "\n",
    "    # Release video object\n",
    "    cap.release()\n",
    "\n",
    "    return stylized_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylized_frames = preprocess_and_display_video(\"temp_video.mp4\", 'Style\\monet.jpeg')\n",
    "    \n",
    "# Create a video from stylized frames using OpenCV's VideoWriter\n",
    "frame_height, frame_width, _ = stylized_frames[0].shape\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(\"stylized_video.mp4\", fourcc, 30, (frame_width, frame_height))\n",
    "\n",
    "for frame in stylized_frames:\n",
    "    out.write(frame)\n",
    "out.release()\n",
    "\n",
    "# Read the stylized video back and display it using Streamlit\n",
    "stylized_video = open(\"stylized_video.mp4\", \"rb\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Style Transfer for real time videos:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_transfer(style_image_path):\n",
    "    ''' This function helps in stylizing videos directly from our camera'''\n",
    "    hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
    "    org_video = cv2.VideoCapture(1)  # This captures the original/real-time videos from the webcam\n",
    "    for webcam_index in [0, 1]:\n",
    "        org_video = cv2.VideoCapture(webcam_index)\n",
    "        if org_video.isOpened():\n",
    "            break\n",
    "            \n",
    "    if not org_video.isOpened():\n",
    "        print('Error reading video file')\n",
    "\n",
    "    # Get the frame_height and width to mirror the actual video we want to stylize\n",
    "    frame_width = int(org_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(org_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(org_video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Preprocess the style image\n",
    "    style_image = plt.imread(style_image_path)\n",
    "    style_image = style_image.astype(np.float32)[np.newaxis, ...] / 255.\n",
    "    style_image = tf.image.resize(style_image, [256, 256])\n",
    "\n",
    "    # Create VideoWriter object\n",
    "    out = cv2.VideoWriter('stylized_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    while org_video.isOpened():\n",
    "      ret, frame = org_video.read()\n",
    "      if not ret:\n",
    "        break\n",
    "\n",
    "    # preprocess the video\n",
    "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      frame = frame.astype(np.float32)[np.newaxis, ...] / 255\n",
    "      frame = tf.image.resize(frame, [256, 256])\n",
    "\n",
    "    # Apply stylization\n",
    "      stylized_output = hub_module(tf.constant(frame), tf.constant(style_image, dtype=tf.float32))\n",
    "      stylized_video_output = stylized_output[0].numpy()[0]\n",
    "\n",
    "    # postprocess the video\n",
    "      stylized_video_output = (stylized_video_output * 255).astype(np.uint8)\n",
    "      stylized_video_output = cv2.cvtColor(stylized_video_output, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "      out.write(stylized_video_output)\n",
    "    \n",
    "    # Display the stylized video in real-time\n",
    "      cv2.imshow('Stylized Video', stylized_video_output)\n",
    "    \n",
    "    # Check if the user pressed the 'q' key to quit\n",
    "      if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "         break\n",
    "    org_video.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print('Video successfully stylized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video successfully stylized\n"
     ]
    }
   ],
   "source": [
    "real_time_transfer(\"Style\\monet.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
